\documentclass[aspectratio=169,10pt]{beamer}
\usepackage{teaching_slides}


\title{Mergers and Counterfactual Prices}
\author{Chris Conlon}
\institute{Grad IO}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Merger Simulation: Two Options}
 \begin{itemize}
\item Partial Merger Simulation
 \begin{itemize}
\item Simulate a new price for $p_j$ after acquiring good $k$ holding the prices of all other goods $(p_k,p_{-j})$ fixed.
\item Repeat for $p_k$ and all other products involved in the merger.
\item Compare price increases to \alert{synergies} or cost savings.
 \end{itemize}
 \item Full Merger Simulation
 \begin{itemize}
\item Write down the full system of post-merger FOC.
\item Adjust post-merger marginal costs for potential synergies.
\item Solve for all prices at the new (post-merger) equilibrium $(p_j,p_k,p_{-j})$.
 \end{itemize}
 \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Differentiated Products Bertrand}
\small
Recall the multi-product Bertrand FOCs:
\begin{align*}
\arg \max_{p \in \calJ_f} \pi_f (\symbf{p}) &= \sum_{j \in \calJ_f} (p_j - c_j) \cdot q_j(\symbf{p}) \\
\rightarrow 0&= q_j(\symbf{p}) + \sum_{k \in \calJ_f} (p_k - c_k) \frac{\partial q_{k}}{\partial p_j}(\symbf{p})
\end{align*}
It is helpful to define the matrix $\Delta$ with entries:
\begin{align*}
\Delta_{(j,k)}(\symbf{p}) = \left\{\begin{array}{lr}
         - \frac{\partial q_{j}}{\partial p_k}(\symbf{p}) & \text{for }  (j,k) \in \calJ_f\\
       	  \quad 0 & \text{for } (j,k) \notin \calJ_f
        \end{array} \right\}
\end{align*}
We can re-write the FOC in matrix form:
\begin{align*}
q(\symbf{p}) = \Delta(\symbf{p})\cdot(\symbf{p}-\symbf{mc})
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Merger Simulation}
What does a merger do? \alert{change the ownership matrix}.
\begin{itemize}

\item Step 1: Recover marginal costs $\widehat{\symbf{mc}} = \symbf{p} +\Delta(\symbf{p})^{-1}q(\symbf{p})$.
\item Step 1a: (Possibly) adjust marginal cost $\widehat{\symbf{mc}}\cdot (1-e)$ with some cost efficiency $e$.
\item Step 2: Change the ownership matrix $\Delta^{pre}(\symbf{p}) \rightarrow \Delta^{post}(\symbf{p})$.
\item Step 3: Solve for $\symbf{p}^{post}$ via: $\symbf{p} = \widehat{\symbf{mc}} - \Delta(\symbf{p})^{-1}q(\symbf{p})$.
\end{itemize}
\pause
\vspace{0.5cm}
\begin{itemize}
\item The first step is easy (just a matrix inverse).
\item The second step is trivial.
\item The third step is tricky because we have to solve an implicit system of equations. $\symbf{p}$ is on both sides.
\end{itemize}
\end{frame}


\begin{frame}{Partial Merger Analysis}
\begin{itemize}
\item Hold all other prices $p_{-j}$ fixed at \alert{pre-merger} prices.
\item Adjust the marginal costs for potential efficiencies.
\item Consider only the FOC for product $j$
\begin{eqnarray*}
0&=& q_j(\symbf{p}) + \sum_{k \in \calJ_f} (p_k - c_k) \frac{\partial q_{k}}{\partial p_j}(\symbf{p})
\end{eqnarray*}
\item Solve for the new $p_j$ given the change in the products controlled by firm $f$: $\calJ_f \rightarrow \calJ_f'$
\item This is a single Gauss-Jacobi step (only products involved in merger).
\end{itemize}
\end{frame} 

\begin{frame}{Partial Merger Analysis: Why bother?}
\begin{itemize}
\item We only need own and cross elasticities for products involved in the merger.
\item Tends to show smaller price increases than full equilibrium merger analysis.
\item Only solving a single equation rather than a system of $J$ nonlinear equations.
\end{itemize}
\end{frame} 



\begin{frame}{Solution Methods}
How do we solve: $\symbf{p} = \widehat{\symbf{mc}} - \Delta(\symbf{p})^{-1}q(\symbf{p})$?
\begin{enumerate}
\item Gauss Jacobi: Simultaneous Best Reply  $p_j^{k+1}(\symbf{p_{-j}^k})$.
\item Gauss Seidel: Iterated Best Response  $p_j^{k+1}(\symbf{p_{<j}^{k+1}},\symbf{p_{>j}^{k}})$.
\item Newton's Method: Set $\symbf{p} - \widehat{\symbf{mc}} + \Delta(\symbf{p})^{-1}q(\symbf{p})=0$\\ but requires derivatives of $\Delta(\symbf{p})^{-1}q(\symbf{p})$
\item Fixed point iteration: $\symbf{p} \mapsfrom \widehat{\symbf{mc}} - \Delta(\symbf{p})^{-1}q(\symbf{p})$
\begin{itemize}
\item Turns out this is \alert{not a contraction}.
\item But you can get lucky... $\symbf{p} - \widehat{\symbf{mc}} + \Delta(\symbf{p})^{-1}q(\symbf{p})=0$ means you have satisfied FOC's
\end{itemize}
\item Alternative fixed point.
\end{enumerate}
\end{frame} 


\begin{frame}{Solution Methods}
\vspace{0.5cm}
General problem $F(\symbf{x}) = 0$ or $m$ nonlinear equations and $m$ unknowns $\symbf{x} = (x_1,\ldots, x_m) \in \mathbb{R}^m$.
\begin{eqnarray*}
F_1 (x_1,\ldots, x_m)  &=& 0 \\
F_2 (x_1,\ldots, x_m)  &=& 0\\
&\vdots&\\ 
F_{N-1} (x_1,\ldots, x_m)  &=& 0\\
F_N (x_1,\ldots, x_m)  &=& 0\\
\end{eqnarray*}
\end{frame} 

\begin{frame}{Solution Methods}
Helpful to write $F(\symbf{x}) = 0 \Leftrightarrow \symbf{x} - \alpha F(\symbf{x}) = \symbf{x}$ which yields the fixed point problem:
\begin{eqnarray*}
G(\symbf{x}) = \symbf{x} -\alpha F(\symbf{x})
\end{eqnarray*}
Fixed point iteration
\begin{eqnarray*}
\symbf{x^{n+1}} = G(\symbf{x^n})
\end{eqnarray*}
Nonlinear Richardson iteration or Picard iteration.\\
\vspace{0.5cm}
We need $G$ to be a \alert{contraction mapping} for iterative methods to guarantee a unique solution (often need strong monotonicity as well).
\end{frame} 

\begin{frame}{Gauss Jacobi: Simultaneous Best Reply}
Current iterate: $\symbf{x^n} = (x_1^n,x_2^n,\ldots,x_{m-1}^n,x_m^n)$.\\
\vspace{0.5cm}
Compute the next iterate $\alert{x^{n+1}}$ by solving one equation in one variable using only values from $\symbf{x^n}$ : 
\begin{eqnarray*}
F_1 (\alert{x_1^{n+1}},x_2^n \ldots, x_{m-1}^n, x_m^n)  &=& 0 \\
F_2  (x_1^n,\alert{x_2^{n+1}},\ldots,x_{m-1}^n,x_m^n)  &=& 0\\
&\vdots&\\ 
F_{m-1}  (x_1^n,x_2^n,\ldots,\alert{x_{m-1}^{n+1}},x_m^n)  &=& 0\\
F_m  (x_1^n,x_2^n,\ldots,x_{m-1}^n, \alert{x_m^{n+1}})  &=& 0
\end{eqnarray*}
Requires contraction and strong monotonicity.
\end{frame} 

\begin{frame}{Gauss Seidel: Iterated Best Response}
Current iterate: $\symbf{x^n} = (x_1^n,x_2^n,\ldots,x_{m-1}^n,x_m^n)$.\\
\vspace{0.5cm}
Compute the next iterate $\alert{\symbf{x^{n+1}}}$ by solving one equation in one variable updating as we go through:
\begin{align*}
F_1 (\alert{x_1^{n+1}},x_2^n \ldots, x_{m-1}^n, x_m^n)  &= 0 \\
F_2  (\alert{x_1^{n+1},x_2^{n+1}},\ldots,x_{m-1}^n,x_m^n)  &= 0\\
&\vdots\\ 
F_{m-1}  (\alert{x_1^{n+1},x_2^{n+1},\ldots,x_{m-1}^{n+1}},x_m^n)  &= 0\\
F_m (\alert{x_1^{n+1},x_2^{n+1},\ldots,x_{m-1}^{n+1}, x_m^{n+1}})  &= 0
\end{align*}
Requires contraction and strong monotonicity.\\
You can speed things up (sometimes) by re-ordering equations.

\end{frame} 

\begin{frame}{Newton-Raphson Method}
\begin{enumerate}
\item Take an initial guess $\symbf{x^0}$
\item Take a Newton step by solving the following system of linear equations
\begin{eqnarray*}
J_F(\symbf{x^n}) \symbf{s^n} = - F(\symbf{x^n})
\end{eqnarray*}
\item New guess $\symbf{x^{n+1}} =\symbf{x^n} + \symbf{s^n}$ or  $\symbf{x^{n+1}} =\symbf{x^n} - J_F^{-1}(\symbf{x^n})\cdot F(\symbf{x^n}) $ 
\item Good (Quadratic) Local convergence
\end{enumerate}
\begin{itemize}
\item Requires $J_F$ (Jacobian) to be Lipschitz continuous. 
\item Linearity means we do not need to take the inverse to solve the system\\
 (just QR decomp -- \texttt{backslash} in MATLAB).
\item Non-singularity of $J_F$ is weaker than strong monotonicity (more like PSD).
\end{itemize}
\end{frame} 

\begin{frame}{Why not always do Newton-Raphson?}
\begin{itemize}
\item Often computing or inverting $J_f(\symbf{x^n})$ is hard.
\item Alternatives focus on simplified ways to compute $J_f(\symbf{x^n})$ or to update $J_f^{-1}(\symbf{x^n})$
\begin{itemize}
\item Some techniques similar to \alert{secant method} (Broyden's Method).
\item Also what are known as \alert{quasi-Newton} methods.
\end{itemize}
\item If NR is feasible: start with that!
\end{itemize}
\end{frame} 

\begin{frame}{Broyden's Method}
Idea: approximate the Jacobian $J_f(\symbf{x^n}) \approx A_n$
\begin{enumerate}
\item Start with $A_0 = \symbf{I}_m$.
\item Iterate on $\symbf{x^{n+1}}= \symbf{x^n} - A_n^{-1} F\left(\symbf{x^{n}}\right)$
\item Update the Jacobian:
\begin{align*}
A_{n+1}=A_{n}-\frac{F\left(\alert{\symbf{x^{n+1}}}\right)\left[A_{n}^{-1}  F\left(\symbf{x^{n}}\right) \right]^{\prime}}{\left[A_{n} F\left(\symbf{x^{n}}\right) \right]^{\prime}\left[A_{n} F\left(\symbf{x^{n}}\right) \right]}
\end{align*}
\end{enumerate}
This is meant to be the multivariate version of the \alert{secant method}.
\end{frame} 


\begin{frame}{Exploit the logit formula}
For the logit the $\Delta$ matrix (for a single market) looks like:
\begin{align*}
\Delta_{(j,k)}(\symbf{p}) = \left\{\begin{array}{lr}
       \int \alpha_i \cdot s_{ij}\cdot (1-s_{ij}) \, \partial F_i& \text{ if } j=k \\
       	  \quad -\int \alpha_i \cdot s_{ij} \cdot s_{ik} \, \partial F_i &  \text{ if }  j\neq k 
        \end{array} \right\}
\end{align*}
Which we can factor into two parts (for plain logit):
\begin{align*}
\Delta(\symbf{p}) =  \underbrace{\text{ Diag}\left[ \alpha \, \symbf{s}(\symbf{p}) \, \right]}_{\Lambda(\symbf{p})} -  \underbrace{\alpha \cdot \symbf{s}(\symbf{p}) \symbf{s}(\symbf{p})'}_{\Gamma(\symbf{p})}
\end{align*}
$\Gamma(\symbf{p})$ and $\Lambda(\symbf{p})$ are $J \times J$ matrices and $\Lambda(\symbf{p})$ is diagonal and $(j,k)$ is nonzero in $\Gamma(\symbf{p})$ only if $(j,k)$ share an owner.
\end{frame}


\begin{frame}{Morrow Skerlos (2010) Fixed Point}
\begin{itemize}
\item After factoring we can rescale by $\Lambda^{-1} (\symbf{p})$
\begin{align*}
(\symbf{p}-\symbf{mc} ) \mapsfrom \Lambda^{-1}(\symbf{p}) \cdot \Gamma(\symbf{p})\cdot(\symbf{p}- \symbf{mc}) - \Lambda^{-1}(\symbf{p})\cdot s(\symbf{p})
\end{align*}
\item This alternative fixed point is in fact a contraction.
\item Moreover the rate of convergence is generally fast and stable (much more than Gauss-Seidel or Gauss-Jacobi).
\item Honestly, this is the best way to solve large pricing games. It nearly always wins and doesn't require derivatives.
\item Coincidentally, this is what \texttt{PyBLP} defaults to.
\end{itemize}
\end{frame}



\begin{frame}{Other Counterfactuals}
Lots of cases where we want to recompute prices:
\begin{itemize}
\item Change conduct/ownership $\mathcal{H}_t(\kappa)$.
\item Change the $mc$ (add a tax, tariff, etc.)
\item Add/drop a product/products from the choice set $\calJ_t$.
\item Change demographics of consumers $d_i$
\end{itemize}
\end{frame}










\end{document}