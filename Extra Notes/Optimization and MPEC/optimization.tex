\input{../../resources/preamble.tex}

\newcommand{\fp}{\frame[plain]}


\title{Bonus Lecture: Nonlinear Optimization}
\author{Chris Conlon  }
\institute{Grad IO}
\date{\today }
\setbeamerfont{equation}{size=\tiny}
\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Basic Setup}
Often we are interested in solving a problem like this:
\begin{description}
\item[Root Finding] $f(x) = 0 $
\item[Optimization] $\arg \min_x f(x)$.
\end{description}
These problems are related because we find the minimum by setting: $f'(x)=0$
\end{frame}


\section{Nonlinear Optimization}
\begin{frame}{Newton's Method for Root Finding}
Consider the Taylor series for $f(x)$ approximated around $f(x_0)$:
\begin{align*}
f(x) \approx f(x_0) + f'(x_0) \cdot (x-x_0) + f''(x_0) \cdot (x-x_0)^2 + o_p(3)
\end{align*}
Suppose we wanted to find a \alert{root} of the equation where $f(x^{*})=0$ and solve for $x$:
\begin{align*}
0 &= f(x_0) + f'(x_0) \cdot (x-x_0) \\
x_1 &= x_0-\frac{f(x_0)}{f'(x_0)} 
\end{align*}
This gives us an \alert{iterative} scheme to find $x^{*}$:
\begin{enumerate}
\item Start with some $x_k$. Calculate $f(x_k),f'(x_k)$
\item Update using $x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)} $
\item Stop when $|x_{k+1}-x_{k}| < \epsilon_{tol}$.
\end{enumerate}
\end{frame}

\begin{frame}{Newton-Raphson for Minimization}
We can re-write \alert{optimization} as \alert{root finding};
\begin{itemize}
\item We want to know $\hat{\theta} = \arg \max_{\theta} \ell(\theta)$.
\item Construct the FOCs $\frac{\partial \ell}{\partial \theta}=0 \rightarrow$  and find the zeros.
\item How? using Newton's method! Set $f(\theta) = \frac{\partial \ell}{\partial \theta}$
\end{itemize}
\begin{align*}
\theta_{k+1} &= \theta_k -  \left[ \frac{\partial^2 \ell}{\partial \theta^2}(\theta_k) \right]^{-1} \cdot \frac{\partial \ell}{\partial \theta}(\theta_k)
\end{align*}
The SOC is that $ \frac{\partial^2 \ell}{\partial \theta^2} >0$. Ideally at all $\theta_k$.\\
This is all for a \alert{single variable} but the \alert{multivariate} version is basically the same.
\end{frame}


\begin{frame}{Newton's Method: Multivariate}
Start with the objective $Q(\theta) = - l(\theta)$:
\begin{itemize}
\item Approximate $Q(\theta)$ around some initial guess $\theta_0$ with a quadratic function
\item Minimize the quadratic function (because that is easy) call that $\theta_1$
\item Update the approximation and repeat.
\begin{align*}
\theta_{k+1} = \theta_k - \left[ \frac{\partial^2 Q}{\partial \theta \partial \theta'} \right]^{-1}\frac{\partial Q}{\partial \theta}(\theta_k)
\end{align*}
\item The equivalent SOC is that the {Hessian Matrix} is \alert{positive semi-definite}  (ideally at all $\theta$).
\item In that case the problem is \alert{globally convex} and has a \alert{unique maximum} that is easy to find.
\end{itemize}
\end{frame}


\begin{frame}{Newton's Method}
We can generalize to Quasi-Newton methods:
\begin{align*}
\theta_{k+1} = \theta_k -  \lambda_k \underbrace{\left[ \frac{\partial^2 Q}{\partial \theta \partial \theta'} \right]^{-1}}_{A_k} \frac{\partial Q}{\partial \theta}(\theta_k)
\end{align*}
Two Choices:
\begin{itemize}
\item Step length $\lambda_k$
\item Step direction $d_k=A_k \frac{\partial Q}{\partial \theta}(\theta_k)$
\item Often rescale the direction to be unit length $\frac{d_k}{\norm{d_k}}$.
\item If we use $A_k$ as the true Hessian and $\lambda_k=1$ this is a \alert{full Newton step}.
\end{itemize}
\end{frame}

\begin{frame}{Newton's Method: Alternatives}
Choices for $A_k$
\begin{itemize}
\item $A_k= I_{k}$ (Identity) is known as \alert{gradient descent} or \alert{steepest descent}
\item BHHH. Specific to MLE. Exploits the \alert{Fisher Information}.
\begin{align*}
A _ { k } 
&= \left[ \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \frac { \partial \ln f } { \partial \theta } \left( \theta _ { k } \right) \frac { \partial \ln f } { \partial \theta ^ { \prime } } \left( \theta _ { k } \right) \right] ^ { - 1 }\\
&=- \mathbb { E } \left[ \frac { \partial ^ { 2 } \ln f } { \partial \theta \partial \theta ^ { \prime } } \left( Z , \theta ^ { * } \right) \right] 
= \mathbb { E } \left[ \frac { \partial \ln f } { \partial \theta } \left( Z , \theta ^ { * } \right) \frac { \partial \ln f } { \partial \theta ^ { \prime } } \left( Z , \theta ^ { * } \right) \right]
\end{align*}
\item Alternatives \alert{SR1} and \alert{DFP} rely on an initial estimate of the Hessian matrix and then approximate an update to $A_k$.
\item Usually updating the Hessian is the costly step.
\item Non invertible Hessians are bad news.
\item Optimizers Differ in: (1) Hessian update; (2) Step-size calculation; (3) Handling of Constraints (which I ignored!)
\end{itemize}
\end{frame}

\section{What about ML Models?}

\begin{frame}{How do people fit ML models?}
\begin{itemize}
	\item Inverting the Hessian for models with large numbers of parameters becomes infeasible.
	\item Many ML models (not all) can be highly \alert{non-convex} meaning that Hessian may be not invertible, and there may be many potential \alert{local minima}.
	\item Most models do some form of \alert{gradient descent} (setting Hessian $A_k=\mathbb{I}$)
	\item Many models use \alert{early stopping}, terminating at a point before we reach the minimum (!)
\end{itemize}
\begin{align*}
\theta_{k+1} = \theta_k - \lambda_k \cdot \frac{\partial Q}{\partial \theta}(\theta_k)
\end{align*}
With key modificiations:
\begin{itemize}
	\item Set the \alert{learning rate / step size} to a small value $\lambda_k < .001$.
	\item Use \alert{batched gradients} to approximate the gradient on a random subset of observations.
	\begin{align*}
	g_k=\frac{1}{\left|\mathcal{B}_t\right|} \sum_{i \in \mathcal{B}_k} \nabla_\theta \ell_i\left(\theta_t\right) .
	\end{align*}
\end{itemize}
This typically means many more iterations than quasi-Newton but often cheaper ones.
\end{frame}

\begin{frame}{AdamW: Objective and Moment Estimates}
We aim to minimize the loss function $\ell(\theta), \quad \theta \in \mathbb{R}^d$.

Gradient and moment estimates (using a batched gradient):
\begin{align}
    g_k &= \nabla_\theta \ell(\theta_k), \\
    m_k &= \beta_1 m_{k-1} + (1 - \beta_1) g_k \quad &\text{(momentum)}, \\
    v_k &= \beta_2 v_{k-1} + (1 - \beta_2) g_k^2 \quad &\text{(adaptive scaling)}.
\end{align}

Bias correction:
\begin{align}
    \hat{m}_k &= \frac{m_k}{1 - \beta_1^k}, \\
    \hat{v}_k &= \frac{v_k}{1 - \beta_2^k}.
\end{align}

\end{frame}

\begin{frame}{AdamW: Update Rule and Properties}
Parameter update:
\begin{align}
    \theta_{k+1} 
    &= \theta_k 
    - \eta \frac{\hat{k}_k}{\sqrt{\hat{v}_k} + \epsilon}
    - \eta \lambda \theta_k.
\end{align}

Key features:
\begin{itemize}
    \item \textbf{Momentum:} $\hat{m}_t$ accumulates past gradients, accelerating learning.
    \item \textbf{Parameter-specific learning rates:} $\hat{v}_t$ rescales each coordinate of $\theta$.
    \begin{itemize}
    	\item This captures how $g_t,g_t^2$ changes from the moving average $m_{k-1},v_{k-1}$ which gives us some estimate of curvature that is parameter specific without trying to approximate $A_k$
    \end{itemize}
    \item \textbf{Decoupled weight decay:} $-\eta \lambda \theta_k$ is applied separately from gradient update.
\end{itemize}
\end{frame}

\begin{frame}{Applications of AdamW}
\texttt{AdamW} is now the standard optimizer in many deep learning frameworks (\texttt{PyTorch}, \texttt{TensorFlow}) and implementations exist for others like Google's \texttt{Jax}. It is often used on very large models
\begin{itemize}
    \item \textbf{Deep learning:} training deep neural networks with millions or billions of parameters.
    \item \textbf{Transformer models:} BERT, GPT, and other large language models often use AdamW by default.
\end{itemize}
But it has some disadvantages
\begin{itemize}
	\item Even on simple problems it may take tens of thousands of iterations. (Quasi-Newton approaches might take a dozen or fewer on convex MLE problems).
	\item The idea is that with \alert{Automatic Differentiation} and \alert{batched gradients} those iterations might be much cheaper.
\end{itemize}



\vspace{0.3cm}
\centering
\end{frame}



\end{document}


