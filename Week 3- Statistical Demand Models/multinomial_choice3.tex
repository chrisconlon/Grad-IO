\input{../resources/preamble2.tex}

\begin{document}
\title{Multinomial Discrete Choice: Mixed Logit}
\author{Chris Conlon}
\institute{Grad IO}
\date{Fall 2025}

\frame{\titlepage}

%\begin{frame}
%\frametitle{Multinomial Logit: IIA}
%The multinomial logit is frequently criticized for producing unrealistic substitution patterns
%\begin{itemize}
%\item Suppose we got rid of a product $k$ then $s_j(\mathcal{J}\setminus k) = s_j(\mathcal{J})\cdot \frac{1}{1- s_k}$.
%\item Substitution is just proportional to your pre-existing shares $s_j$
%\item No concept of ``closeness'' of competition!
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Can we do better?}
%Multinomial Probit?
%\begin{itemize}
%\item The probit has $\varepsilon_i \sim N(0,\Sigma)$.
%\item If $\Sigma$ is unrestricted, then this can produce relatively flexible substitution patterns.
%\item Flexible is relative: still have normal tails, only pairwise correlations, etc.
%\item It might be that $\rho_{12}$ is large if $1,2$ are similar products.
%\item Much more flexible than Logit
%\end{itemize}
%Downside
%\begin{itemize}
%\item $\Sigma$ has potentially $J^2$ parameters (that is a lot)!
%\item Maybe $J * (J-1)/2$ under symmetry. (still a lot).
%\item Each time we want to compute $s_j(\theta)$ we have to simulate an integral of dimension $J$.
%\item I wouldn't do this for $J \geq 5$.
%\end{itemize}
%\end{frame}

\begin{frame}{Mixed/ Random Coefficients Logit}
An alternative is to allow for individuals to have \alert{random coefficients} $\beta_i$:
\begin{eqnarray*}
U_{ij} = \beta_i x_{j} +  \varepsilon_{ij}, \quad \beta_i \sim f( \beta_i | \theta),  \quad \varepsilon_{ij} \sim \text{Type I EV}
\end{eqnarray*}

As an alternative, we could have specified an error components structure on $\varepsilon_i$.
\begin{align*}
U_{ij} = \beta \, x_{j} \, \textrm{y}_i + \underbrace{x_{j} \, \nu_i  + \varepsilon_{ij}}_{\tilde{\varepsilon}_{ij}}
\end{align*}
\vspace{-0.8cm}
\begin{itemize}
\item $x_{j}$ are observed per usual, $\textrm{y}_i$ are individual demographics/features, and $\varepsilon_{ij}$ is IID Type I EV.
\item The key is that $\nu_i$ is unobserved and mean zero (often normally distributed).
\item This allows for a heteroskedastic structure on $\boldsymbol{\varepsilon_{i}}$, but only one which we can project down onto the space of $x$.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Mixed Logit}
We relax the IIA property by mixing over various logits:
\begin{align*}
\beta_i &= \overline{\beta} + \Pi \, \textrm{y}_i  + \Sigma \, \nu_i   
&\mu_{ij}=&   x_{j} \cdot \left(  \Pi \, \textrm{y}_i + \Sigma \, \nu_{i} \right) \\
u_{ijt} &= x_j \beta_i + \varepsilon_{ij}  
&=&  x_j \beta  + \mu_{ij}+ \varepsilon_{ij}   \\
s_{ij}(\theta) &= \int \frac{\exp[x_{j} \beta_i ]}{1+\sum_k \exp[x_{k} \beta_i ]} f(\beta_i | \theta) 
&=& \int \frac{\exp[x_{j} \beta + \mu_{ij} ]}{1+\sum_k \exp[x_{k} \beta+ \mu_{ik} ]} f(\boldsymbol{\mu_i} | \theta)
\end{align*}
 \begin{itemize}
 \item Each individual draws a vector $\beta_i\boldsymbol{/\mu_i}$ (separately from $\boldsymbol{\varepsilon_i}$).
 \item Conditional on $\beta_i\boldsymbol{/\mu_i}$ each person follows an IIA logit model.
 \item However we integrate (or mix) over many such individuals giving us a \alert{mixed logit} or \alert{heirarchical model} (if you are a statistician)
 \item In practice these are not that different from linear \alert{random effects models} you have learned about in econometrics.
 \item It helps to think about fixing $\beta_i\boldsymbol{/\mu_i}$ first and then integrating out over $\varepsilon_i$
 \end{itemize}
\end{frame}


%
%\begin{frame}{Mixed/ Random Coefficients Logit}
%For each individual $i$, the resulting choice probability follows a logit:
%\begin{eqnarray*}
%P_{ij} = \int \frac{ e^{V_{ij}(\beta_i)}}{\sum_k e^{V_{ik}(\beta_i)}} f(\beta_i | \theta) \partial \beta
%\end{eqnarray*}
%This structure is quite general:
%\begin{itemize}
%\item The choice probabilities are know a function of unknown parameters $\theta$.
%\item We can allow for there to be two types of $\beta_i$ in the population (high-type, low-type). \alert{latent class model}.
%\item We can allow $\beta_i$ to follow an independent normal distribution for each component of $x_{ij}$ such as $\beta_i = \overline{\beta} + \nu_i \sigma$.
%\item We can allow for correlated normal draws using the Cholesky root of the covariance matrix.
%\item Can allow for non-normal distributions too (lognormal, exponential). Why is normal so easy?
%\end{itemize}
%\end{frame}

\begin{frame}{Kinds of heterogeneity}
\begin{align*}
s_{ij}(\theta) &= \int \frac{\exp[x_{j} \beta_i ]}{1+\sum_k \exp[x_{k} \beta_i ]} f(\boldsymbol{\beta_i} | \theta) \approx \sum_{s=1}^S w_i^s \, \frac{\exp[x_{j} \beta_i^s ]}{1+\sum_k \exp[x_{k} \beta_i^s ]}
\end{align*}

\begin{itemize}
\item We can only \alert{approximate} the integral with \alert{weights} $w_{i}^s$ and \alert{nodes} $\beta_i^s$
\begin{itemize}
\item We can allow for there to be two types of $\beta_i$ in the population (high-type, low-type). \alert{latent class model}.
\item We can allow $\beta_i$ to follow an independent normal distribution for each component of $x_{ij}$ such as $\beta_i = \overline{\beta} + \nu_i \sigma$.
\item We can allow for correlated normal draws $\beta_i \sim N(\mu_{\beta},\Sigma_{\beta})$.
\item Can allow for non-normal distributions too (lognormal, exponential).
\item Why is normal so easy?
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Adding Heterogeneity}
\begin{itemize}
\item The structure is extremely flexible but at a cost.
\item We generally must perform the integration numerically.
\item High-dimensional numerical integration is difficult. In fact, integration in dimension 8 or higher makes me very nervous.
\item We need to be parsimonious in how many variables have unobservable heterogeneity.
\item Again observed heterogeneity does not make life difficult so the more of that the better!
\begin{itemize}
\item If we see individual income, education, distance to hospital, etc. we can always interact that with observed characteristics without doing any additional integration.
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Mixed Logit}
How does it work?
 \begin{itemize}
\item Well we are mixing over individuals who conditional on $\beta_i$ or $\boldsymbol{\mu_i}$ follow logit substitution patterns, however they may differ wildly in their $s_{ij}$ and hence their substitution patterns.
\item For example if we are buying cameras: I may care a lot about price, you may care a lot about megapixels, and someone else may care mostly about zoom.
\item The basic idea is that we need to explain the heteroskedasticity of $Cov(\boldsymbol{\varepsilon_i}, \boldsymbol{\varepsilon_{\ell}},)$ what random coefficients do is let us use a basis from our $X$'s.
\item If our $X$'s are able to span the space effectively, then an RC logit model can approximate any arbitrary RUM (such as probit) (McFadden and Train 2002). 
\item Of course if you have 1000 products and two random coefficients, you are asking for a lot.
 \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Elasticities}
At the level of an individual substitution patterns follow a plain logit:
\begin{align*}
\epsilon_{s_j,p_j} = \frac{\partial s_{j}}{\partial p_j} \cdot \frac{p_j}{s_{j}}
=\frac{p_j}{s_{j}}  \cdot \int \frac{\partial s_{ij}}{\partial p_j}  \, d F_i 
= \frac{p_j}{s_{j}}  \int \beta_i \cdot s_{ij} \cdot (1-s_{ij} ) \,d F_i 
\end{align*}
\begin{align*}
\epsilon_{s_j,p_k} = \frac{\partial s_{j}}{\partial p_k} \cdot \frac{p_k}{s_{j}}
=\frac{p_k}{s_{j}}  \cdot \int \frac{\partial s_{ij}}{\partial p_k} \,  d F_i 
=- \frac{p_k}{s_{j}}  \int \beta_i \cdot s_{ij}\cdot s_{ik}\, d F_i 
\end{align*}
Notation: here $s_j \equiv s_j(x_i)$ denotes an individual choice probability (we re-use the $i$ subscript) and $s_{ij}(\boldsymbol{\mu_{i}})=s_{ij}$.
\end{frame}





\begin{frame}
\frametitle{Substitution (Conlon Mortimer 2020)}
At the level of an individual substitution patterns follow a plain logit:
\begin{align*}
D_{jk,i}(x) = \frac{\frac{\partial s_{ik}}{\partial p_j}}{\left| \frac{\partial s_{ij}}{\partial p_j} \right|} = \frac{s_{ik}(x)}{1-s_{ij}(x)}
\end{align*}
But at the aggregate level, we mix over heterogeneous individuals:
\begin{align*}
D_{jk}(x) = \int D_{jk,i}(x) w_i(z_j,z_j',x) d F_i \quad  \text{ where }
\quad w_i(z_j,z_j',x) = \frac{q_{ij}(z_j',x)-q_{ij}(z_j,x)}{q_{j}(z_j',x)-q_{j}(z_j,x)}
\end{align*}
Different interventions (price changes, product removals, quality changes, etc.) give different weighting schemes.
\end{frame}


\begin{frame}
\frametitle{Estimation Details: MSL/MSLE}
How do we estimate these models? (Derived from multinomial log-likelihood):
\begin{align*}
\theta_{MLE} = \arg \min_{\theta} - \sum_{i=1}^N \sum_{j=1}^J y_{ij} \ln \widehat{s}_{ij}(\theta)
\end{align*}

\begin{itemize}
\item We need to perform numerical integration to get $\widehat{s}_{ij}(\theta)$ and its derivative: $\widehat{\frac{\partial s_{ij}}{\partial \theta}}$.
\item Consistency
\begin{itemize}
\item Technically for fixed number of MC draws the MSLE estimator is inconsistent. Why?
\item Very accurate integration is even more important!
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Estimation Alternatives: MSM}
Can also do a simulated GMM estimator with moment conditions:
\begin{align*}
E\left[\left(y_{ij} - \widehat{s}_{ij}(\theta) \right) | z_{ij} \right]=0
\end{align*}
\begin{itemize}
\item We need to perform numerical integration to get $\widehat{s}_{ij}(\theta)$ and its derivative: $\widehat{\frac{\partial s_{ij}}{\partial \theta}}$.
\item The \alert{optimal instruments} in the sense of Chamberlain (1987) or Amemiya (1977) are the derivative of moments with respect to parameters (\alert{scores}): $z_{ij} = \frac{\partial \log s_{ij}}{\partial \theta}(\theta)$.
\item The true scores are infeasible (they depend on $\theta_0$).
\begin{itemize}
\item At true scores: same FOC as MLE.
\item Simulated scores have same consistency issue as MSLE.
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Estimation Alternatives: MSS}
As an alternative we could work with the score function directly:
\begin{align*}
\sum_{i=1}^n\widehat { \frac{\partial \log s_{ij}}{\partial \theta}}(\theta) =\sum_{i=1}^N \frac{1}{\widehat{s_{ij}}(\theta)} \cdot  \widehat{\frac{\partial s_{ij}}{\partial \theta}}(\theta) = 0
\end{align*}
\begin{itemize}
\item We can get unbiased estimated of derivative (linearity)
\item We can't get unbiased estimate of $\frac{1}{\widehat{s_{ij}}(\theta)}$
\begin{itemize}
\item Train's book suggests Accept-Reject 
\item Maybe importance sampling would work?
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Estimation Details: Hints}
 How bad is the simulation error?
 \begin{itemize}
\item Depends how small your shares are. 
\item Since you care about $\log s_{jt}$ when shares are small, tiny errors can be enormous.
\item Often it is pretty bad.
%\item I recommend sticking with quadrature at a high level of precision.
%\item \texttt{sparse-grids.de} provide efficient high dimensional quadrature rules.
 \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Estimation Details: Recommendations}
You should read these separate notes, but my recommendations are:
\begin{enumerate}
\item Numerical integration: 
\begin{itemize}
\item Use Gauss-Hermite quadrature rules to integrate over normal densities.
\item Follow Heiss and Winschel (JoE) and do \alert{sparse grids} in medium dimensions \item Try \url{http://sparse-grids.de}.
\end{itemize}
\item Nonlinear optimization: with mixtures the problem is non-convex (ie: very hard)
\begin{itemize}
\item You \alert{must} provide analytic gradients.
\item You should use a quasi-Newton solver and may need to try several.
\item Try multiple starting values.
\end{itemize}
\item Generalized Method of Moments
\begin{itemize}
\item You can read more about optimal IV in these settings.
\end{itemize}
\end{enumerate}
\end{frame}



%\subsection{A Semiparametric Estimator}
%
%\begin{frame}
%\frametitle{Even More Flexibility (Fox, Kim, Ryan, Bajari)}
%Suppose we wanted to nonparametrically estimate $f(\beta_i | \theta)$ instead of assuming that it is normal or log-normal.
%\begin{eqnarray*}
%s_{ij} &=& \int \frac{\exp[x_{j} \beta_i  ]}{1+\sum_k \exp[x_{k} \beta_i  ]} f(\beta_i | \theta)
%\end{eqnarray*}
%\begin{itemize}
%\item Choose a distribution $g(\beta_i)$ that is more spread out that $f(\beta_i | \theta)$
%\item Draw several $\beta_{s}$ from that distribution (maybe 500-1000).
%\item Compute $\hat{s}_{ij}(\beta_s)$ for each draw of $\beta_s$ and each $j$.
%\item Holding $\hat{s}_{ij}(\beta_s)$ fixed, look for $w_s$ that solve
%\begin{eqnarray*}
%\min_w \left(s_j -  \sum_{s=1}^{ns} w_s \hat{s}_{ij}(\beta_s) \right)^2 \quad \mbox{ s.t. } \sum_{s=1}^{ns} w_s = 1, \quad w_s \geq 0 \quad \forall s
%\end{eqnarray*}
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Even More Flexibility (Fox, Kim, Ryan, Bajari)}
%% Hey this is lasso
%\begin{itemize}
%\item Like other semi-/non- parametric estimators, when it works it is both general and very easy.
%\item We are solving a least squares problem with constraints: positive coefficients, coefficients sum to 1.
%\item It tends to produce \alert{sparse models} with only a small number of $\beta_s$ getting positive weights.
%\item This is way easier than solving a random coefficients logit model with all but the simplest distributions.
%\item There is a bias-variance tradeoff in choosing $g(\beta_i)$.
%\item Incorporating parameters that are not random coefficients loses some of the simplicity.
%\item I have no idea how to do this with large numbers of fixed effects.
%\end{itemize}
%\end{frame}
%
%

%\begin{frame}{Mixed/ Random Coefficients Logit}
%How do we approximate:
%\begin{eqnarray*}
%P_{ij} = \int \frac{ e^{V_{ij}(\beta_i)}}{\sum_k e^{V_{ik}(\beta_i)}} f(\beta_i | \theta) \partial \beta
%\end{eqnarray*}
%
%\begin{itemize}
%\item Monte Carlo Integration
%\begin{itemize}
%\item Draw $\beta_i$ from the candidate distribution. $[\beta_i^{(1)}, \beta_i^{(2)}, \ldots\beta_i^{(s)}] | \theta$.
%\item For each $\beta_i$ calculate $P_{ij}(\beta_i)$.
%\item $\frac{1}{S} \sum_{s=1}^S P_{ij} = \widehat{P_{j}^{s}}$
%\end{itemize}
%\end{itemize}
%The way we usually get correlated normal variables (or any normal variables) is to transform independent normals appropriately.
%\end{frame}

%\begin{frame}{In Practice}
%Suppose there is only one normally distributed random coefficient:
%\begin{itemize}
%\item $f(\beta_i \theta) \sim N(\overline{\beta},\sigma)$.
%\item We can re-write this as the integral over a transformed standard normal density
%\begin{eqnarray*}
%s_{ij}(\theta) = \int \frac{ e^{V_{ij}(\nu_i,\theta)}}{\sum_k e^{V_{ik}(\nu_i,\theta)}} f(\nu_i) \partial \nu
%\end{eqnarray*}
%\item Monte Carlo Integration: Independent Normal Case
%\begin{itemize}
%\item Draw $\nu_i$ from the standard normal distribution.
%\item Now we can rewrite $\beta_i = \overline{\beta} + \nu_i \sigma$
%\item For each $\beta_i$ calculate $s_{ij}(\beta_i)$.
%\item $\frac{1}{S} \sum_{s=1}^S s_{ij} = \widehat{s_{j}^{s}}$
%\end{itemize}
%\item Gaussian Quadrature
%\begin{itemize}
%\item Or we can draw a non-random set of points $\nu_i$ and corresponding weights $w_i$ and approximate the integral to a high level of polynomial accuracy.
%\end{itemize}
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Quadrature in higher dimensions}
%\begin{itemize}
%\item Quadrature is great in low dimensions -- but scales badly in high dimensions.
%\item If we need $N_a$ points to accurately approximate the integral in $d=1$ then we need $N_a^d$ points in dimension $d$ (using the tensor product of quadrature rules).
%\item There is some research on quadrature rules that nest and also how to carefully eliminate points so that the number doesn't grow so quickly.
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Estimation}
%How do we actually estimate these models?
%\begin{itemize}
%\item In practice we should be able to do MLE.
%\begin{eqnarray*}
%\max_{\theta} \sum_{i=1}^N y_{ij} \log s_{ij}(\theta)
%\end{eqnarray*}
%\item When we are doing IIA logit, this problem is globally convex and is easy to estimate using Newton's Method.
%\item When doing nested logit or random coefficients logit, it generally is non-convex which can make life difficult.
%\item The tough part is generally working out what $\frac{\partial \log s_{ij}}{\partial \theta}$ is, especially when we need to simulate to obtain $s_{ij}$.
%\item It turns out that MSLE actually has consistent problems for fixed $S$. Why?
%\item Alternative? MSM/MoM type estimators
%\end{itemize}
%\end{frame}
%
%
%
%
%%\begin{frame}
%%\frametitle{Mixed Logit}
%% \begin{itemize}
%%\item Now the choice of $\mu_{ij}$ is crucial.  
%%\item A popular choice is \alert{random coefficients logit}: $\beta_i = \overline{\beta} +  \Sigma \nu_i * \mathbf{x_j}$ where $\nu_i$ is a vector of standard normal draws and $\Sigma$ are covariance parameters so that $\beta_i \sim N(\overline{\beta},\Sigma)$.
%%\item We have to estimate $(\overline{\beta},\Sigma)$.
%%\item People are often lazy and let $\Sigma$ be a diagonal matrix to reduce the \# of parameters.
%% \end{itemize}
%%\end{frame}
%
%
%\begin{frame}
%\frametitle{Mixed Logit: Estimation}
% \begin{itemize}
%\item Just like before, we do MLE
%\item One wrinkle--how do we compute the integral?
% \end{itemize}
%\begin{eqnarray*}
%s_{ij} &=& \int \frac{\exp[x_{j} \beta_i  ]}{1+\sum_k \exp[x_{k} \beta_i  ]} f(\beta_i | \theta) \approx \sum_{s=1}^{ns} w_s \frac{\exp[x_{j} (\overline{\beta} + \Sigma \nu_{is})  ]}{1+\sum_k \exp[x_{k} (\overline{\beta} + \Sigma \nu_{is})  ]} 
%\end{eqnarray*}
% \begin{itemize}
%\item Option 1: Monte Carlo integration.  Draw $NS=1000$ or so samples of $\nu_i$ from the standard normal and set $w_i = \frac{1}{NS}$.
%\item Option 2: Quadrature. Choose $\nu_i$ and $w_i$ according to a Gaussian quadrature rule. Like \texttt{quad} in MATLAB or \texttt{mvquad} in R.
%\item Personally I get nervous about integrals in dimension greater than 5. People routinely have 20 or more though.
% \end{itemize}
%\end{frame}
%
%
%
%
%
%\section{Convexity and Computation}
%\frame{\frametitle{Convexity}
%\begin{block}{An optimization problem is convex if}
%\begin{eqnarray*}
%\min_{x} f(\mathbf{x}) &s.t.& h(\mathbf{x}) \leq 0 \quad A \mathbf{x} = 0
%\end{eqnarray*}
%\vskip -2ex
%\begin{itemize}
%\item $f(\mathbf{x}),h(\mathbf{x)}$ are convex (PSD second derivative matrix)
%\item Equality Constraint is affine
%\end{itemize}
%\end{block}
%\begin{exampleblock}{Some helpful identities about convexity}
%\begin{itemize}
%\item Compositions  and sums of convex functions are convex.
%\item Norms $||$ are convex, $\max$ is convex, $\log$ is convex
%\item  $ \log(\sum_{i=1}^n \exp(x_i))$ is convex.
%\item Fixed Points can introduce non-convexities.
%\item Globally convex problems have a unique optimum
%\end{itemize}
%\end{exampleblock}
%}
%
%\frame{\frametitle{Properties of Convex Optimization}
%\begin{itemize}
%\item If a program is globally convex then it has a unique minimizer that will be found by convex optimizers.
%\item If a program is not globally convex, but is convex over a region of the parameter space, then most convex optimization routines find any local minima in the convex hull 
%\item Convex optimization routines are unlikely to find local minima (including the global minimum) if they do not begin in the same convex hull as the optimum (starting values matter!).
%\item Most good commercial routines are clever about dealing with multiple starting values and handling problems that are well approximated by convex functions.
%\item Good Routines use information about sparseness of Hessian -- this generally determines speed.
%\end{itemize}
%}
%
%\subsection{Nested Logit Example}
%
%
%%\subsection{Logit and Nested Logit}
%%\frame{\frametitle{Logit Model}
%%Easy to see that FIML estimator is convex:
%%\begin{eqnarray*}
%%\min_{\theta} \sum_j q_j \ln \left(\frac{\exp[x_j \beta ]}{1+\sum_j \exp[x_j \beta]} \right)\\
%%\min_{\theta} \sum_j q_j \left( x_j \beta  - \ln \left(1+\sum_j \exp[x_j \beta ]\right) \right)\\
%%\end{eqnarray*}
%%}
%
%\frame{\frametitle{Nested Logit Model}
%\begin{exampleblock}{FIML Nested Logit Model is Non-Convex}
%\begin{eqnarray*}
%\min_{\theta} \sum_j q_j \ln S_j(\theta) \quad \mbox{s.t.} \quad  S_j(\theta) = \frac{e^{x_j \beta/ \lambda}( \sum_{k \in g_l} e^{x_j \beta/ \lambda})^{\lambda-1}}{\sum_{\forall l'} ( \sum_{k \in g_l'} e^{x_j \beta/ \lambda})^{\lambda} }
%\end{eqnarray*}
%This is a pain to show but the problem is with the cross term $\frac{\partial^2 S_j}{\partial \beta \partial \lambda}$ because $\exp[x_j \beta / \lambda]$ is not convex.
%\end{exampleblock}
%\begin{exampleblock}{A Simple Substitution Saves the Day:  let $\gamma = \beta / \lambda$}
%\begin{eqnarray*}
%\min_{\theta} \sum_j q_j \ln S_j(\theta) \quad \mbox{s.t.} \quad  
%S_j(\theta) = \frac{e^{x_j \gamma}( \sum_{k \in g_l} e^{x_j \gamma})^{\lambda-1}}{\sum_{\forall l'} ( \sum_{k \in g_l'} e^{x_j \gamma})^{\lambda} }
%\end{eqnarray*}
%This is much better behaved and easier to optimize.
%\end{exampleblock}
%}
%
%\frame{\frametitle{Nested Logit Model}
%\begin{center}
%\rowcolors[]{1}{RoyalBlue!10}{RoyalBlue!20} 
%\begin{tabular}{lrrr}
%&\bf{Original}\footnote{KNITRO-AMPL} & \bf{Substitution}\footnote{KNITRO-AMPL} & \bf{No Derivatives}\footnote{fminunc-MATLAB} \\
%Parameters &49& 49&49\\
%Nonlinear $\lambda$ &5 &5 &5\\
%Likelihood & 2.279448 &2.279448  & 2.27972\\
%Iterations &197 &146 & 352\\
%Time & 59.0 s & 10.7 s & 192s  \\
%\end{tabular}
%\end{center}
%Discuss Nelder-Meade
%}
%
%\frame{\frametitle{Computing Derivatives}
%A key aspect of any optimization problem is going to be computing the derivatives (first and second) of the model.  There are some different approaches
%\begin{itemize}
%\item Numerical: Often inaccurate and error prone (why?)
%\item Pencil and Paper: this tends to be mistake prone -- but often actually the fastest
%\item Automatic (AMPL): Software brute forces through a chain rule calculation at every step (limited language).
%\item Symbolic (Maple/Mathematica): software ``knows'' derivatives of certain objects and can do its own simplification.  (limited language).
%\end{itemize}
%}
%
\end{document}
